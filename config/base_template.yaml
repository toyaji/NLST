log:
  name: '20210916'
  version: 'test_1'
  log_graph: False

dataset:
  train_data: ['DIV2K', 'Flickr2K', 'BSD500',]                 # DIV2K, Flickr2K, BSD500, ...
  test_data: ['Set5', 'Set14', ]                              # ['Set5', 'Set14', 'BSD100', 'Urban100', 'Manga109']
  save_test_img: False
  test_only: False
  batch_size: 4
  shuffle: True
  num_workers: 5
  args:
    dir: "dataset"
    patch_size: 64                       # for NLST training, it should be a multiple of reduction size option
    scale: 2
    rgb_range: 1                         # HAN, SwinIR pretrained as of 255, others as of 1

model:
  net: 'SCAN'
  pretrain: True
  loss: 'l1'       

trainer:                                 # pytorch lightning trainer options https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags           
  gpus: 1
  max_epochs: 100
  accumulate_grad_batches: 2
  gradient_clip_val: 0.5
  log_gpu_memory: "all"
  #limit_val_batches: 0.5
  #profiler: "pytorch"
  #resume_from_checkpoint: 'logs/nonscan_2x_10block5attn/log_2111071414/checkpoints/epoch=15-step=3503.ckpt'
  #fast_dev_run: 1              

callback:
  save_top_k: 5
  earlly_stop_patience: 20
  min_delta: 1.0e-4

optimizer:
  name: 'adamw'
  learning_rate: 1.0e-4
  weight_decay: 1.0e-4
  momentum: 0.9                         # only work for SGD

scheduler:
  name: 'multistep'
  min_lr: 1.0e-8
  multistep: [20, 40, 60, 80]       # work for MultiStepLR
  patience: 10                          # work for RedcueLROnPlateau
  cooldown: 5                           # work for RedcueLROnPlateau, skippping given epoch after one step down
  period: 5                             # work for cosine shceduler, period of back to start lr
  factor: 0.1                           # lr schedule decay factor
  