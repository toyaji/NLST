log:
  name: '20210916'
  version: 'test_1'
  log_graph: False

dataset:
  train_data: ["DIV2K"]              
  test_data: ['Set5', 'Set14', 'BSD100', 'Urban100', 'Manga109']
  batch_size: 12
  shuffle: True
  num_workers: 16
  test_only: False
  args:
    dir: "dataset"
    patch_size: 96
    scale: 2

model:
  net: 'HAN'       
  pretrain: True     
  scale: 2
  n_resgroups: 10
  n_resblocks: 20
  n_feats: 64
  reduction: 16
  rgb_range: 1
  n_colors: 3
  res_scale: 1

trainer:                       
  gpus: 1
  max_epochs: 250
  accumulate_grad_batches: 1
  gradient_clip_val: 0.5
  #check_val_every_n_epoch: 5   # this is not working with earllystop callback
  limit_val_batches: 0.5
  #profiler: "pytorch"
  #resume_from_checkpoint: "logs/adamW07_rgb01_v2/20210914/checkpoints/epoch=98-step=6236.ckpt"
  #fast_dev_run: 1          # This is for test - it will cause error for validation step

callback:
  save_top_k: 3
  earlly_stop_patience: 20

optimizer:
  learning_rate: 1.0e-5
  weight_decay: 1.0e-7
  patience: 10
  factor: 0.5                  # lr schedule decay factor
